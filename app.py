import os
import streamlit as st
from dotenv import load_dotenv
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.deepinfra import DeepInfraEmbeddingModel
from llama_index.llms.deepinfra import DeepInfraLLM

# Configuraci√≥n de la aplicaci√≥n Streamlit (Mover a la parte superior)
st.set_page_config(page_title="Asistente de Emergencia", page_icon="üö®", layout="centered")

# Configuraci√≥n de DeepInfra API
# Cargar variables de entorno desde .env
load_dotenv()
deepinfra_api_key = os.getenv("DEEPINFRA_TOKEN")

if not deepinfra_api_key:
    # Cargar la API Key de DeepInfra desde las secrets de Streamlit Cloud
    deepinfra_api_key = st.secrets["DEEPINFRA_TOKEN"]
    
    if not deepinfra_api_key:
        st.error("Falta la API Key de DeepInfra. Config√∫rala en un archivo .env o en el apartado secrets de stramlit cloud")
    

# Definir el modelo de embeddings y LLM de DeepInfra
# Configurar el modelo de embeddings con el nuevo modelo BGAI/bge-m3
Settings.embed_model = DeepInfraEmbeddingModel(
    model_id="BAAI/bge-m3",  # ID del modelo de embeddings de DeepInfra
    api_token=deepinfra_api_key,  # Token de la API
    normalize=True,  # Normalizaci√≥n opcional
    text_prefix="text: ",  # Prefijo de texto
    query_prefix="query: ",  # Prefijo de consulta
)

Settings.chunk_size = 512

# Crear el modelo de embeddings
embedding_model = Settings.embed_model

# Configuraci√≥n centralizada del modelo LLM utilizando Settings
Settings.llm = DeepInfraLLM(
    model="meta-llama/Llama-3.3-70B-Instruct-Turbo",  # Modelo de DeepInfra
    api_key=deepinfra_api_key,  # Tu clave API de DeepInfra
    temperature=0,  # Configuraci√≥n de temperatura para control de creatividad
)

# Funci√≥n para cargar y guardar el √≠ndice y el modelo de embeddings
@st.cache_resource
def load_index():    
    docs_folder = "docs"  # Aseg√∫rate de que tus documentos est√©n en esta carpeta
    documents = SimpleDirectoryReader(docs_folder).load_data()
    
    # Usar DeepInfraEmbeddingModel para el √≠ndice
    index = VectorStoreIndex.from_documents(documents, embed_model=embedding_model)
    return index.as_query_engine(streaming=True, similarity_top_k=1)

# Cargar el √≠ndice y el modelo de embeddings
query_engine = load_index()

# Configuraci√≥n de la conversaci√≥n
promt = "Eres un asistente experto en emergencias llamado AsistencIA. Responde √∫nicamente preguntas relacionadas con la informaci√≥n en los documentos proporcionados. Si la pregunta no est√° cubierta, indica que no puedes responder. Si es necesario, complementa con informaci√≥n de internet, pero solo si est√° estrictamente relacionada con los documentos. No incluyas informaci√≥n no verificada ni hagas referencia a los documentos. Da respuestas claras, precisas y directas, sin explicaciones innecesarias. Intenta dar siempre la soluci√≥n al problema planteado con una respuesta concisa.Proporciona el numero de emergencia siempre que puedas o la lista de n√∫meros que conozcas."

# Saludo
saludo = "üëã ¬°Hola! Soy **AsistencIA**, tu asistente en situaciones de emergencia. Estoy aqu√≠ para ayudarte a resolver cualquier urgencia o aprender qu√© hacer en momentos cr√≠ticos. ¬øEn qu√© puedo ayudarte?"

# Inicializar historial de conversaci√≥n en la sesi√≥n de Streamlit
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "system", "content": promt},
        {"role": "assistant", "content": saludo}
    ]

# Inicializar el estado de la sesi√≥n
if "processing" not in st.session_state:
    st.session_state.processing = False  # Variable de bloqueo para evitar m√∫ltiples consultas simult√°neas

# Configuraci√≥n de la aplicaci√≥n
st.title("üö® Asistente de Emergencia")
st.write("Pregunta sobre emergencias y obt√©n respuestas basadas en documentos.")

# Sidebar de la aplicaci√≥n
with st.sidebar:
    # Bot√≥n "Nuevo chat" para reiniciar la conversaci√≥n
    if st.button("Nuevo chat"):
        st.session_state.messages = [
            {"role": "system", "content": promt},
            {"role": "assistant", "content": saludo}
        ]
        st.session_state.processing = False
    
    # Informaci√≥n sobre la aplicaci√≥n
    st.markdown("---")
    st.markdown("### Descripci√≥n")
    st.write("Este amable asistente de emergencia proporciona respuestas a preguntas basadas en documentos.")
    st.write("Hecho con ‚ù§Ô∏è por los estudiantes de **Accenture**")
    st.markdown("---")

# Mostrar historial de chat
for msg in st.session_state.messages:
    if msg["role"] == "user":
        st.chat_message("user", avatar="üßë‚Äçüíº").write(f"{msg['content']}")
    elif msg["role"] == "assistant":
        st.chat_message("assistant", avatar="ü§ñ").write(f"{msg['content']}")
    
# Entrada del usuario
if user_query := st.chat_input("Escribe tu pregunta...", disabled=st.session_state.processing):
    # Bloquear entrada de usuario durante la consulta
    st.session_state.processing = True
    
    # Agregar la pregunta al historial de mensajes
    st.session_state.messages.append({"role": "user", "content": user_query})
    
    # Mostrar la pregunta en el chat
    st.chat_message("user", avatar="üßë‚Äçüíº").write(f"{user_query}")
    
    # Recargar la interfaz para reflejar el cambio
    st.rerun()
    
# Procesar respuesta en una iteraci√≥n separada despu√©s de que la UI ya se haya actualizado
if st.session_state.processing and len(st.session_state.messages) > 1 and st.session_state.messages[-1]["role"] == "user":
        # Realizar consulta de respuesta en una iteraci√≥n separada
        with st.chat_message("assistant", avatar="ü§ñ"):
            # Crear un contexto con todas las preguntas y respuestas previas
            conversation_history = "\n".join(
                [f"(msg['role'].capitalize()): {msg['content']}" for msg in st.session_state.messages]
            )
            
            # Colocamos el spinner en un contenedor vac√≠o para que se pueda manipular
            spinner_placeholder = st.empty()
            # Colocamos el contenedor de respuesta en un contenedor vac√≠o para que se pueda manipular
            response_placeholder = st.empty()
            # Variable para guardar la respuesta
            response_text = ""

            # Mostrar spinner junto al avatar
            with spinner_placeholder:
                with st.spinner("Pensando..."):
                # Generar respuesta en streaming
                    streaming_response = query_engine.query(f"Historial de conversaci√≥n:\n{conversation_history}\n\nNueva pregunta: {user_query}")
                    
                    # Quitar el spinner despu√©s de completar la operaci√≥n
                    spinner_placeholder.empty()
                    for fragment in streaming_response.response_gen:
                        response_text += fragment  # Acumular texto generado
                        response_placeholder.write(response_text)  # Mostrarlo 

            # Asignar el texto completo como respuesta final
            response = response_text

        # Agregar la respuesta al historial de mensajes
        st.session_state.messages.append({"role": "assistant", "content": response})
        
        # Desbloquear entrada de usuario despu√©s de obtener la consulta
        st.session_state.processing = False
        # Recargar la interfaz para reflejar el cambio
        st.rerun()
